import { NextRequest, NextResponse } from 'next/server'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { searchKnowledgeBase, formatSearchResults } from '@/lib/knowledge-search'

// ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«ã‚’ç’°å¢ƒå¤‰æ•°ã§åˆ‡ã‚Šæ›¿ãˆ
const USE_OLLAMA = process.env.USE_OLLAMA === 'true'
const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || 'http://localhost:11434'
const DEFAULT_OLLAMA_MODEL = process.env.OLLAMA_MODEL || 'gpt-oss:20b'

// RAGæ©Ÿèƒ½ãƒ•ãƒ©ã‚°
const ENABLE_RAG = process.env.ENABLE_RAG === 'true'

// åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
const AVAILABLE_MODELS = [
  { id: 'gpt-oss:20b', name: 'GPT-OSS 20B', provider: 'ollama' },
  { id: 'qwen2.5:7b-instruct-q4_k_m', name: 'Qwen2.5 7B Q4_K_M', provider: 'ollama' },
  { id: 'llama3.1:8b-instruct-q4_k_m', name: 'Llama3.1 8B Q4_K_M', provider: 'ollama' },
  { id: 'gemma3:12b', name: 'Gemma3 12B', provider: 'ollama' },
  { id: 'gemini-1.5-flash', name: 'Gemini 1.5 Flash', provider: 'gemini' }
]

// Gemini APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')

// ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…±é€šåŒ–
const systemPrompt = `ã‚ãªãŸã¯ã€Œç”°ä¸­å·¥æ¥­AIã€ã¨ã„ã†åå‰ã®ã€æ©Ÿæ¢°åŠ å·¥ã¨è£½é€ æ¥­ã«ç‰¹åŒ–ã—ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    
ä»¥ä¸‹ã®ç‰¹å¾´ã‚’æŒã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- æ©Ÿæ¢°åŠ å·¥ï¼ˆæ—‹ç›¤ã€ãƒã‚·ãƒ‹ãƒ³ã‚°ã€æ¨ªä¸­ã€ãƒ©ã‚¸ã‚¢ãƒ«ï¼‰ã®å°‚é–€çŸ¥è­˜ã‚’æŒã¤
- åˆ‡å‰Šæ¡ä»¶ã€å·¥å…·é¸å®šã€åŠ å·¥æ‰‹é †ã«ã¤ã„ã¦è©³ã—ã„
- ç¾å ´ã®ä½œæ¥­è€…ã«åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹
- å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›
- å®‰å…¨æ€§ã‚’å¸¸ã«é‡è¦–
- æ—¥æœ¬ã®è£½é€ æ¥­ã®æ…£ç¿’ã«è©³ã—ã„
- å›ç­”ã¯500æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€è¦ªåˆ‡ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`

// ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹GETã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
export async function GET() {
  return NextResponse.json({ models: AVAILABLE_MODELS })
}

export async function POST(request: NextRequest) {
  try {
    const { messages, model, enableRAG } = await request.json()
    
    // é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
    const selectedModel = AVAILABLE_MODELS.find(m => m.id === model) || AVAILABLE_MODELS[0]
    const useOllamaForThisRequest = selectedModel.provider === 'ollama'
    const modelId = selectedModel.id
    
    // RAGæ©Ÿèƒ½ï¼šç¤¾å†…ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æœ‰åŠ¹åŒ–ï¼‰
    let ragContext = ''
    const shouldUseRAG = ENABLE_RAG || enableRAG
    
    if (shouldUseRAG && messages.length > 0) {
      try {
        const lastMessage = messages[messages.length - 1]
        const conversationHistory = messages.slice(0, -1).map(m => m.content)
        
        console.log('ğŸ” RAGæ¤œç´¢é–‹å§‹:', lastMessage.content)
        const searchResults = await searchKnowledgeBase(lastMessage.content, conversationHistory)
        ragContext = formatSearchResults(searchResults)
        console.log('ğŸ“Š RAGæ¤œç´¢çµæœ:', searchResults.statistics)
      } catch (ragError) {
        console.error('RAGæ¤œç´¢ã‚¨ãƒ©ãƒ¼:', ragError)
        // RAGå¤±æ•—æ™‚ã‚‚é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã¯ç¶™ç¶š
      }
    }
    
    // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
    const enhancedSystemPrompt = ragContext 
      ? `${systemPrompt}\n\n${ragContext}\n\nä¸Šè¨˜ã®ç¤¾å†…ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚‚å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚`
      : systemPrompt

    // Ollamaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    if (useOllamaForThisRequest) {
      try {
        // Ollamaç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
        const ollamaMessages = [
          { role: 'system', content: enhancedSystemPrompt },
          ...messages
        ]

        // Ollama APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        const ollamaResponse = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model: modelId,
            messages: ollamaMessages,
            stream: false
          })
        })

        if (!ollamaResponse.ok) {
          throw new Error(`Ollama API error: ${ollamaResponse.status}`)
        }

        const ollamaData = await ollamaResponse.json()
        return NextResponse.json({ response: ollamaData.message.content })
        
      } catch (ollamaError) {
        console.error('Ollama error:', ollamaError)
        // Ollamaã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—ï¼‰
        return NextResponse.json({ 
          response: `${selectedModel.name} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚\n\nã‚¨ãƒ©ãƒ¼: ${ollamaError instanceof Error ? ollamaError.message : 'Unknown error'}` 
        }, { status: 500 })
      }
    }

    // Geminiã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
    const geminiModel = genAI.getGenerativeModel({ model: modelId })

    // ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
    const conversationHistory = messages.map((msg: any) => 
      `${msg.role === 'user' ? 'ãƒ¦ãƒ¼ã‚¶ãƒ¼' : 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ'}: ${msg.content}`
    ).join('\n\n')

    // Geminiã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    const prompt = `${enhancedSystemPrompt}\n\nä¼šè©±å±¥æ­´:\n${conversationHistory}\n\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:`

    // Geminiã‹ã‚‰ã®å¿œç­”ã‚’å–å¾—
    const result = await geminiModel.generateContent(prompt)
    const response = await result.response
    const text = response.text()

    return NextResponse.json({ response: text })
  } catch (error) {
    console.error('Chat API error:', error)
    
    // APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if (!process.env.GEMINI_API_KEY) {
      return NextResponse.json({ 
        response: 'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆæ©Ÿèƒ½ã¯æº–å‚™ä¸­ã§ã™ã€‚\n\nGemini APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ï¼š\n1. .env.localãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ\n2. GEMINI_API_KEY=your-api-key ã‚’è¿½åŠ \n3. ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•\n\nè©³ã—ãã¯ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚' 
      }, { status: 500 })
    }

    return NextResponse.json({ 
      response: 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãçµŒã£ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚' 
    }, { status: 500 })
  }
}